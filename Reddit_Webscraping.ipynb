{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "aiIz8m0BPCK4",
    "outputId": "c5e82205-919e-41b1-c895-1d40dbca24ff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: praw in c:\\users\\delph\\anaconda3\\lib\\site-packages (7.7.1)\n",
      "Requirement already satisfied: websocket-client>=0.54.0 in c:\\users\\delph\\anaconda3\\lib\\site-packages (from praw) (0.58.0)\n",
      "Requirement already satisfied: update-checker>=0.18 in c:\\users\\delph\\anaconda3\\lib\\site-packages (from praw) (0.18.0)\n",
      "Requirement already satisfied: prawcore<3,>=2.1 in c:\\users\\delph\\anaconda3\\lib\\site-packages (from praw) (2.4.0)\n",
      "Requirement already satisfied: requests<3.0,>=2.6.0 in c:\\users\\delph\\anaconda3\\lib\\site-packages (from prawcore<3,>=2.1->praw) (2.28.1)\n",
      "Requirement already satisfied: six in c:\\users\\delph\\anaconda3\\lib\\site-packages (from websocket-client>=0.54.0->praw) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\delph\\anaconda3\\lib\\site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\delph\\anaconda3\\lib\\site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (1.26.11)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\delph\\anaconda3\\lib\\site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\delph\\anaconda3\\lib\\site-packages (from requests<3.0,>=2.6.0->prawcore<3,>=2.1->praw) (2024.2.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\delph\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\delph\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\delph\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\delph\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\delph\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -orch (c:\\users\\delph\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install praw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "ZHFGX1raPHux"
   },
   "outputs": [],
   "source": [
    "import praw\n",
    "reddit = praw.Reddit(client_id='_59QL953o_8uoVjRd_gb8A', client_secret='31MwY5SMhykt-6ekyDnIxylyVXzQtA', user_agent='nlp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B1BLQ8WMPegr",
    "outputId": "b303f05b-c8f9-4e6e-d246-ec7f23697ad4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This subreddit is about the PSYCHOLOGICAL TECHNIQUES created by Richard Bandler and John Grinder.\n",
      "Self-Promotion Thread 2024\n",
      "can you slide your chair closer to me ..\n",
      "Drug induced abuse victim cannot make sense of what happened, hypnosis involved (Help needed) \n",
      "Modeling: Is feelings one of most essential things to modelate?\n",
      "Is this ok?\n",
      "Ross Jeffries: The Andrew Tate Of The 1990s\n",
      "Resources for phobia treatment\n",
      "Wet Dreams \n",
      "NLP is a pyramidal scheme?\n"
     ]
    }
   ],
   "source": [
    "# get 10 hot posts from the NLP subreddit\n",
    "hot_posts = reddit.subreddit('NLP').hot(limit=10)\n",
    "for post in hot_posts:\n",
    "    print(post.title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nXeSnsOzPmFl",
    "outputId": "e375fde9-5f93-4fec-d1ca-5da1671aef5f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Totally 100% Factual* information published about Elon Musk, who says there is no need for misinformation laws\n",
      "This way he shows proof that it's not sped up.\n",
      "They used to teach typing in school too\n",
      "Quick maths\n",
      "This is not about hunting. Alex Larenty, from Britain, lives in a South African game reserve where he gives lions foot massages. He started doing this after noticing that a cream for paw infections made a lion relax and look happy.\n",
      "Accessing an underground fire hydrant in the UK\n",
      "US buys 81 Soviet-era combat aircraft from Russia's ally for less than $20,000 each, report says\n",
      "20 years ago today CBS News released these pictures showing the torture at Abu Ghraib prison.\n",
      "EKT IS FOUND (HEAVY NSFW WARNING)\n",
      "Need some advice..\n"
     ]
    }
   ],
   "source": [
    "# get hottest posts from all subreddits\n",
    "hot_posts = reddit.subreddit('all').hot(limit=10)\n",
    "for post in hot_posts:\n",
    "    print(post.title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dP9vFwP6Pmtv",
    "outputId": "cb06a7c6-5ac5-4c5d-8916-583fb954183d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               title  score       id  \\\n",
      "0                              Welcome to /r/sports!      0  1bfpxfw   \n",
      "1  Hey, Reddit: We're 100 Days Out to the Paris O...     14  1c5xguh   \n",
      "2  Chiefs owner considers leaving Arrowhead Stadi...   3771  1cexkrv   \n",
      "3  A new study debunks a longstanding medical myt...    653  1cez9ba   \n",
      "4        Lillard out for Game 4 with Achilles injury     68  1cfakvk   \n",
      "5  Sanders won't follow sons to NFL, has 'work to...   1131  1cetz9m   \n",
      "6  LeBron scores 30, and the Lakers avoid 1st-rou...    376  1cewnb1   \n",
      "7  Former Yankee Fritz Peterson, who famously tra...   2496  1cefaac   \n",
      "8   Chargers draft Brenden Rice, son of legendary WR    333  1cesr3k   \n",
      "9   Adam Reynolds gets a Heroic One Legged Intercept    288  1cesfim   \n",
      "\n",
      "  subreddit                                                url  num_comments  \\\n",
      "0    sports  https://www.reddit.com/r/sports/comments/1bfpx...            12   \n",
      "1    sports  /r/olympics/comments/1c5fcjb/hey_reddit_were_1...             1   \n",
      "2    sports  https://sports.yahoo.com/chiefs-owner-says-lea...           849   \n",
      "3    sports  https://www.abc.net.au/news/2024-04-28/study-d...            83   \n",
      "4    sports          https://www.thescore.com/nba/news/2902891            11   \n",
      "5    sports  https://www.espn.com/college-football/story/_/...           127   \n",
      "6    sports  https://apnews.com/article/lakers-nuggets-scor...            45   \n",
      "7    sports  https://www.foxnews.com/sports/former-yankee-f...           195   \n",
      "8    sports          https://www.thescore.com/nfl/news/2900211            32   \n",
      "9    sports                    https://v.redd.it/f546xmlry3xc1            32   \n",
      "\n",
      "                                                body       created  \n",
      "0  [View Post](https://www.reddit.com/r/sports/po...  1.710540e+09  \n",
      "1                                                     1.713318e+09  \n",
      "2                                                     1.714278e+09  \n",
      "3                                                     1.714284e+09  \n",
      "4                                                     1.714322e+09  \n",
      "5                                                     1.714266e+09  \n",
      "6                                                     1.714274e+09  \n",
      "7                                                     1.714227e+09  \n",
      "8                                                     1.714262e+09  \n",
      "9                                                     1.714261e+09  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "posts = []\n",
    "nlp_subreddit = reddit.subreddit('Sports')\n",
    "for post in nlp_subreddit.hot(limit=10):\n",
    "    posts.append([post.title, post.score, post.id, post.subreddit, post.url, post.num_comments, post.selftext, post.created])\n",
    "posts = pd.DataFrame(posts,columns=['title', 'score', 'id', 'subreddit', 'url', 'num_comments', 'body', 'created'])\n",
    "print(posts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0SmQT067QFA6",
    "outputId": "cf7168cd-5fbd-4722-f227-236a8163d862"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:praw:It appears that you are using PRAW in an asynchronous environment.\n",
      "It is strongly recommended to use Async PRAW: https://asyncpraw.readthedocs.io.\n",
      "See https://praw.readthedocs.io/en/latest/getting_started/multiple_instances.html#discord-bots-and-asynchronous-environments for more info.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It's actually pretty difficult NOT to be vague, since probably 99% of time people use vague language (shortened versions - deletions, generalizations, distortions), otherwise it would take hours just to communicate something simple. Most of the time, people will infer the missing details of what you are talking about based on the context.\n",
      "\n",
      "Let's take what I've wrote for example:\n",
      "\n",
      "It's actually pretty difficult - how difficult exactly? Not specified =vague\n",
      "\n",
      "difficult - from what angle, what makes it difficult? Not specified =vague\n",
      "\n",
      "people - which people more specifically?\n",
      "\n",
      "vague - what does vague mean, more specifically?\n",
      "\n",
      "... and I could go on like this for hours. We don't notice vague language / deletions because our brains fill in the missing details unconsciously.\n",
      "\n",
      "Since we are already vague in language most of the time, my question would be \"what do you want to achieve by using vague language\", and the answer might help you filter the existing resources.\n",
      "Learning how to chunk up can help w this. Being fluid with the hierarchy of ideas can be extremely useful, being able to chunk down to specific language and chunk up to vague abstract language. It's something I'm always working on as well. When thinking of vague language, I think about it in terms of agreement. When you chunk down to specific language you give people more reasons to disagree with you, and when you leave a lot of deleted distorted and generalized info in the statement, it becomes vaguer and you gain more agreement, because there's less to disagree with.   \n",
      "\n",
      "\n",
      "To chunk up just ask, what is this an example of?  \n",
      "\n",
      "\n",
      "Australian Shepard -> Dog -> Animal -> Species -> Being   -> etc.\n",
      "There are many new things you can learn... And as you find yourself searching for these things.... You become lost.....lost in a deep sea\n",
      ".... Becoming deeper and deeper.\n",
      "\n",
      "Vague is generalisation, vague is metaphorical, vague is Repetitive and once you become vague then you can lead 😂\n",
      "\n",
      "Whats the objective of what you're trying to do? Is it therapeutic work? Hypnosis? Selling shit? Pursuading people?\n",
      "In simple terms, how true can the statement be = vague. How false can the statement be = specific.\n",
      "\n",
      "Vague example - Throughout the day you are most likely thinking things.\n",
      "\n",
      "This statement is more than likely 100% true.\n",
      "\n",
      "\n",
      "\n",
      "Specific Example - At 1:30 PM I know you were thinking about that hotdog you were going to eat.\n",
      "\n",
      "This statements is absolutely false on the account that I cannot know what you are thinking at the specific time and your intention - not vague and can be false on multiple levels.\n",
      "\n",
      "  \n",
      "Forget the NLP jargon and ask yourself, \"Could this be true?\" The more accuracy you need, the less true it becomes for others.\n",
      "\n",
      "\n",
      "\n",
      "Other examples:\n",
      "\n",
      "There are times when you feel great and then there are times when you feel not-so-great.\n",
      "\n",
      "Sometimes there are thoughts you have that maybe your'd rather not have and might wonder if others have the same thoughts as you.\n",
      "\n",
      "At some point or another you might wonder where you're going next.\n",
      "You don't want vague language. You want the language of influence which is seemingly detailed. All is explained in the online ABC-NLP Practitioner here => [https://www.influence.amsterdam/2021/07/11/free-online-abc-nlp-practitioner/](https://www.influence.amsterdam/2021/07/11/free-online-abc-nlp-practitioner/)\n",
      "\n",
      "Starting with lesson #7.\n",
      "I have a list of my favorite NLP video links\n",
      "Interesting? One of my goals that I am hoping to find assistance with here is to be more precise/deliberate with my communication.\n",
      "Sleight of mouth by Robert Dilts is a good read\n"
     ]
    }
   ],
   "source": [
    "submission = reddit.submission(id=\"1aubsqj\")\n",
    "for top_level_comment in submission.comments:\n",
    "    print(top_level_comment.body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8Lozc4lcQXE7",
    "outputId": "068b885a-7a36-414c-bb79-c432763b33a7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to /r/sports!\n",
      "\n",
      "Hey, Reddit: We're 100 Days Out to the Paris Olympics and Paralympics and doing an AMA with Olympians, Paralympians and hopefuls this Wednesday!\n",
      "\n",
      "Chiefs owner considers leaving Arrowhead Stadium after sales tax funding was rejected\n",
      "\n",
      "A new study debunks a longstanding medical myth - that a torn ACL can’t heal without surgery.\n",
      "\n",
      "Lillard out for Game 4 with Achilles injury\n",
      "\n"
     ]
    }
   ],
   "source": [
    "subreddit = reddit.subreddit(\"Sports\")\n",
    "for post in subreddit.hot(limit=5):\n",
    "    print(post.title)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 570
    },
    "id": "E6V79h89RnEX",
    "outputId": "c280fd1f-e369-4c05-cbc0-a342480e3218"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\delph\\AppData\\Local\\Temp\\ipykernel_6848\\165352984.py:3: DeprecationWarning: Positional arguments for 'BaseListingMixin.top' will no longer be supported in PRAW 8.\n",
      "Call this function with 'time_filter' as a keyword argument.\n",
      "  posts = subreddit.top(\"month\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Caitlin Clark inking 8-year, $28M deal with Nike</td>\n",
       "      <td>1cbdtx7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Caitlin Clark Jersey Out-Sells Entire Dallas C...</td>\n",
       "      <td>1c9vg5d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Rob Gronkowski with the first pitch for the Re...</td>\n",
       "      <td>1c4scgu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Elly De La Cruz with an inside the park home run!</td>\n",
       "      <td>1bzfc5j</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Jets QB Aaron Rodgers says U.S. Government cre...</td>\n",
       "      <td>1c615bb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Reggie Bush plans to continue with defamation ...</td>\n",
       "      <td>1cd428x</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Bob Cole, the play-by-play voice of countless ...</td>\n",
       "      <td>1ccwsy9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Player scores a three pointer from his own bas...</td>\n",
       "      <td>1bzig5o</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Feds say Shohei Ohtani's ex-interpreter stole ...</td>\n",
       "      <td>1c1ltf5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>This year's WNBA Draft viewership reached 2.45...</td>\n",
       "      <td>1c5ridc</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Title       ID\n",
       "0    Caitlin Clark inking 8-year, $28M deal with Nike  1cbdtx7\n",
       "1   Caitlin Clark Jersey Out-Sells Entire Dallas C...  1c9vg5d\n",
       "2   Rob Gronkowski with the first pitch for the Re...  1c4scgu\n",
       "3   Elly De La Cruz with an inside the park home run!  1bzfc5j\n",
       "4   Jets QB Aaron Rodgers says U.S. Government cre...  1c615bb\n",
       "..                                                ...      ...\n",
       "95  Reggie Bush plans to continue with defamation ...  1cd428x\n",
       "96  Bob Cole, the play-by-play voice of countless ...  1ccwsy9\n",
       "97  Player scores a three pointer from his own bas...  1bzig5o\n",
       "98  Feds say Shohei Ohtani's ex-interpreter stole ...  1c1ltf5\n",
       "99  This year's WNBA Draft viewership reached 2.45...  1c5ridc\n",
       "\n",
       "[100 rows x 2 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Scraping the top posts of the current month in sports\n",
    "subreddit = reddit.subreddit(\"Sports\")\n",
    "posts = subreddit.top(\"month\")\n",
    "posts_dict = {\"Title\": [], \"ID\": [] }\n",
    "\n",
    "for post in posts:\n",
    "    posts_dict[\"Title\"].append(post.title)\n",
    "    posts_dict[\"ID\"].append(post.id)\n",
    "    \n",
    "top_posts = pd.DataFrame(posts_dict)\n",
    "top_posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "CcqSPy7pSPiV"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "top_posts.to_csv(\"Top Posts_sports.csv\", index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "w9xOy-BlFB_Y"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\delph\\AppData\\Local\\Temp\\ipykernel_6848\\904131890.py:3: DeprecationWarning: Positional arguments for 'BaseListingMixin.top' will no longer be supported in PRAW 8.\n",
      "Call this function with 'time_filter' as a keyword argument.\n",
      "  posts = subreddit.top(\"week\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Owen Wilson reportedly turned down $12 million...</td>\n",
       "      <td>1ccc2lx</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Emma Stone Reveals She 'Would Like to Be' Call...</td>\n",
       "      <td>1cd4cbc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Megan Thee Stallion accused of harassment by c...</td>\n",
       "      <td>1cb8vw8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Doja Cat scolds parents who bring kids to her ...</td>\n",
       "      <td>1cdulcp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Jimmy Kimmel thanks hospital staff and support...</td>\n",
       "      <td>1caqh4j</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Jeremy Clarkson Reveals His Previous Dismissal...</td>\n",
       "      <td>1cf7n4f</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Bob Bakish Out as Paramount Global CEO</td>\n",
       "      <td>1ceq5ui</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>Encyclopedia Brown: A Story for My Brother, Ph...</td>\n",
       "      <td>1caopuf</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Eminem Dropping New Album The Death of Slim Sh...</td>\n",
       "      <td>1cdgyj2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Nicki Minaj Throws Item Back into Crowd After ...</td>\n",
       "      <td>1caes9s</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Title       ID\n",
       "0   Owen Wilson reportedly turned down $12 million...  1ccc2lx\n",
       "1   Emma Stone Reveals She 'Would Like to Be' Call...  1cd4cbc\n",
       "2   Megan Thee Stallion accused of harassment by c...  1cb8vw8\n",
       "3   Doja Cat scolds parents who bring kids to her ...  1cdulcp\n",
       "4   Jimmy Kimmel thanks hospital staff and support...  1caqh4j\n",
       "..                                                ...      ...\n",
       "95  Jeremy Clarkson Reveals His Previous Dismissal...  1cf7n4f\n",
       "96             Bob Bakish Out as Paramount Global CEO  1ceq5ui\n",
       "97  Encyclopedia Brown: A Story for My Brother, Ph...  1caopuf\n",
       "98  Eminem Dropping New Album The Death of Slim Sh...  1cdgyj2\n",
       "99  Nicki Minaj Throws Item Back into Crowd After ...  1caes9s\n",
       "\n",
       "[100 rows x 2 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Scraping the top posts of the current week in Entertainment\n",
    "subreddit = reddit.subreddit(\"Entertainment\")\n",
    "posts = subreddit.top(\"week\")\n",
    "posts_dict = {\"Title\": [], \"ID\": [] }\n",
    "\n",
    "for post in posts:\n",
    "    posts_dict[\"Title\"].append(post.title)\n",
    "    posts_dict[\"ID\"].append(post.id)\n",
    "    \n",
    "top_posts = pd.DataFrame(posts_dict)\n",
    "top_posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_posts.to_csv(\"Top Posts_entertainment.csv\", index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "nA8elaf0pF01"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\delph\\AppData\\Local\\Temp\\ipykernel_6848\\236506386.py:3: DeprecationWarning: Positional arguments for 'BaseListingMixin.top' will no longer be supported in PRAW 8.\n",
      "Call this function with 'time_filter' as a keyword argument.\n",
      "  posts = subreddit.top(\"week\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Title</th>\n",
       "      <th>ID</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Kids are actively and willingly skipping reces...</td>\n",
       "      <td>1ccgixd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Why do we finish high school at 18?</td>\n",
       "      <td>1ceh1hl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Failed Class for AI Assumption</td>\n",
       "      <td>1ceo58j</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>I was just talking to a teen the other day who...</td>\n",
       "      <td>1ca6axb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>PSSA testing in Pennsylvania</td>\n",
       "      <td>1cdx6tw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Immigrant English teacher?</td>\n",
       "      <td>1cdc0tc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Push in pull out and intervention programs MTSS</td>\n",
       "      <td>1cdw0jd</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Very broad question, but what are some academi...</td>\n",
       "      <td>1cbkt79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>This is ridiculous</td>\n",
       "      <td>1ceztww</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>How to become well-read?</td>\n",
       "      <td>1cezn1a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Does a person's success depends on university ...</td>\n",
       "      <td>1ceyzvr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Company Wants To Address Euro Teacher Shortage...</td>\n",
       "      <td>1cdi6go</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Why has my preschooler STAR test score gone do...</td>\n",
       "      <td>1cd53ob</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>Test on Saturday Morning</td>\n",
       "      <td>1cawv1e</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>First Grade Classroom Management</td>\n",
       "      <td>1c9zxa9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>Want to study in EU but I don't know how to.</td>\n",
       "      <td>1cf2bxt</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Tips for studying?</td>\n",
       "      <td>1cdtapy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Any benefits of an Associates in Automotive?</td>\n",
       "      <td>1cdg8m7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>What opportunities does a 35 on the ACT open f...</td>\n",
       "      <td>1cd4vds</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>does second degree students socially interact ...</td>\n",
       "      <td>1cczywk</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>Ideas for things to teach to elder people</td>\n",
       "      <td>1ccm7yc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>Didn't Graduate Highschool</td>\n",
       "      <td>1ccc3si</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>Passage of a Kansas City, Kansas, school bond ...</td>\n",
       "      <td>1cc3n3a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>Kindergarten Start for Children Born September...</td>\n",
       "      <td>1cbcr0g</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>education resources?</td>\n",
       "      <td>1cbck09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>What are some tips for starting online school?</td>\n",
       "      <td>1caji6b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>Would like to study stress management/ fight o...</td>\n",
       "      <td>1cfcsip</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>Sped case manager</td>\n",
       "      <td>1cdvjyc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>Interview dates clashing</td>\n",
       "      <td>1cdkijb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>Text to speech</td>\n",
       "      <td>1cd9ito</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Paid Study Opportunity</td>\n",
       "      <td>1cd8pa4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>What is okay to even expect??</td>\n",
       "      <td>1cd4yxb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>Can I use FASFA to get a lineman and HVAC Tech...</td>\n",
       "      <td>1ccitct</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>Seeking Recommendations for Computer-Aided Ins...</td>\n",
       "      <td>1cc5rrc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>Summer jobs</td>\n",
       "      <td>1cb95ko</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>Trying to choose a science major</td>\n",
       "      <td>1casli8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>Please take this academic survey</td>\n",
       "      <td>1ca292m</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>Can an adult not in high school take AP exams?</td>\n",
       "      <td>1ca1g72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>[Academic] Survey</td>\n",
       "      <td>1c9xjx8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>They ask that more budget funds be allocated t...</td>\n",
       "      <td>1cetx6q</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>Register for Python Bootcamp</td>\n",
       "      <td>1cdpi2e</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>frustrated with finding an art school</td>\n",
       "      <td>1c9pyf5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>scholarships for international students</td>\n",
       "      <td>1cf7u1y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>I just need to get this out there</td>\n",
       "      <td>1c9z4g5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>brainly</td>\n",
       "      <td>1cek1n6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>Dream</td>\n",
       "      <td>1ce16q3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>How To Email College Admissions</td>\n",
       "      <td>1cc77zw</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>Gifted and Talented</td>\n",
       "      <td>1caxxjb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>A fully online bachelor's degree that is compl...</td>\n",
       "      <td>1cb5s28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>Apparently school has been purging on subs or ...</td>\n",
       "      <td>1ce48vq</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>School system broken?</td>\n",
       "      <td>1cao4kq</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>Controversial Take// Early Years</td>\n",
       "      <td>1ccd0cr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>Teaching and helping students with ADHD - Some...</td>\n",
       "      <td>1cb4n7x</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>A Critical Examination of Education in America</td>\n",
       "      <td>1cbxm4l</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Title       ID\n",
       "0   Kids are actively and willingly skipping reces...  1ccgixd\n",
       "1                 Why do we finish high school at 18?  1ceh1hl\n",
       "2                     Failed Class for AI Assumption   1ceo58j\n",
       "3   I was just talking to a teen the other day who...  1ca6axb\n",
       "4                        PSSA testing in Pennsylvania  1cdx6tw\n",
       "5                          Immigrant English teacher?  1cdc0tc\n",
       "6     Push in pull out and intervention programs MTSS  1cdw0jd\n",
       "7   Very broad question, but what are some academi...  1cbkt79\n",
       "8                                  This is ridiculous  1ceztww\n",
       "9                            How to become well-read?  1cezn1a\n",
       "10  Does a person's success depends on university ...  1ceyzvr\n",
       "11  Company Wants To Address Euro Teacher Shortage...  1cdi6go\n",
       "12  Why has my preschooler STAR test score gone do...  1cd53ob\n",
       "13                           Test on Saturday Morning  1cawv1e\n",
       "14                  First Grade Classroom Management   1c9zxa9\n",
       "15       Want to study in EU but I don't know how to.  1cf2bxt\n",
       "16                                 Tips for studying?  1cdtapy\n",
       "17       Any benefits of an Associates in Automotive?  1cdg8m7\n",
       "18  What opportunities does a 35 on the ACT open f...  1cd4vds\n",
       "19  does second degree students socially interact ...  1cczywk\n",
       "20          Ideas for things to teach to elder people  1ccm7yc\n",
       "21                         Didn't Graduate Highschool  1ccc3si\n",
       "22  Passage of a Kansas City, Kansas, school bond ...  1cc3n3a\n",
       "23  Kindergarten Start for Children Born September...  1cbcr0g\n",
       "24                              education resources?   1cbck09\n",
       "25     What are some tips for starting online school?  1caji6b\n",
       "26  Would like to study stress management/ fight o...  1cfcsip\n",
       "27                                  Sped case manager  1cdvjyc\n",
       "28                           Interview dates clashing  1cdkijb\n",
       "29                                    Text to speech   1cd9ito\n",
       "30                             Paid Study Opportunity  1cd8pa4\n",
       "31                      What is okay to even expect??  1cd4yxb\n",
       "32  Can I use FASFA to get a lineman and HVAC Tech...  1ccitct\n",
       "33  Seeking Recommendations for Computer-Aided Ins...  1cc5rrc\n",
       "34                                        Summer jobs  1cb95ko\n",
       "35                   Trying to choose a science major  1casli8\n",
       "36                   Please take this academic survey  1ca292m\n",
       "37     Can an adult not in high school take AP exams?  1ca1g72\n",
       "38                                  [Academic] Survey  1c9xjx8\n",
       "39  They ask that more budget funds be allocated t...  1cetx6q\n",
       "40                       Register for Python Bootcamp  1cdpi2e\n",
       "41              frustrated with finding an art school  1c9pyf5\n",
       "42            scholarships for international students  1cf7u1y\n",
       "43                  I just need to get this out there  1c9z4g5\n",
       "44                                            brainly  1cek1n6\n",
       "45                                              Dream  1ce16q3\n",
       "46                    How To Email College Admissions  1cc77zw\n",
       "47                                Gifted and Talented  1caxxjb\n",
       "48  A fully online bachelor's degree that is compl...  1cb5s28\n",
       "49  Apparently school has been purging on subs or ...  1ce48vq\n",
       "50                              School system broken?  1cao4kq\n",
       "51                  Controversial Take// Early Years   1ccd0cr\n",
       "52  Teaching and helping students with ADHD - Some...  1cb4n7x\n",
       "53     A Critical Examination of Education in America  1cbxm4l"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Scraping the top posts of the current week in Education\n",
    "subreddit = reddit.subreddit(\"Education\")\n",
    "posts = subreddit.top(\"week\")\n",
    "posts_dict = {\"Title\": [], \"ID\": [] }\n",
    "\n",
    "for post in posts:\n",
    "    posts_dict[\"Title\"].append(post.title)\n",
    "    posts_dict[\"ID\"].append(post.id)\n",
    "    \n",
    "top_posts = pd.DataFrame(posts_dict)\n",
    "top_posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_posts.to_csv(\"Top Posts_education.csv\", index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\delph\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\delph\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\delph\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download NLTK resources (run once)\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    # Remove special characters and numbers\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "\n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text.lower())\n",
    "\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_tokens = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "    # Lemmatize the tokens\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(word) for word in filtered_tokens]\n",
    "\n",
    "    # Join the tokens back into a single string\n",
    "    clean_text = ' '.join(lemmatized_tokens)\n",
    "\n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Top Posts_sports.csv')\n",
    "\n",
    "df['cleaned_text'] = df['Title'].apply(clean_text)\n",
    "\n",
    "df.to_csv('cleaned_data_sports.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Top Posts_entertainment.csv')\n",
    "\n",
    "df['cleaned_text'] = df['Title'].apply(clean_text)\n",
    "\n",
    "df.to_csv('cleaned_data_entertainment.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Top Posts_education.csv')\n",
    "\n",
    "df['cleaned_text'] = df['Title'].apply(clean_text)\n",
    "\n",
    "df.to_csv('cleaned_data_education.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "import string\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subreddit to scrape\n",
    "subreddit = reddit.subreddit('Sports')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\delph\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\delph\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Download NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Get stopwords and punctuation\n",
    "stop_words = set(stopwords.words('english'))\n",
    "punctuation = set(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_comments(subreddit, limit=10):\n",
    "    comments_emotions = defaultdict(list)\n",
    "    for submission in subreddit.hot(limit=limit):\n",
    "        submission.comments.replace_more(limit=None)\n",
    "        for comment in submission.comments.list():\n",
    "            text = comment.body\n",
    "            preprocessed_text = preprocess_text(text)\n",
    "            inputs = tokenizer(preprocessed_text, return_tensors=\"pt\")\n",
    "            outputs = model(**inputs)\n",
    "            logits = outputs.logits\n",
    "            predicted_class_idx = torch.argmax(logits, dim=1).item()\n",
    "            predicted_class = model.config.id2label[predicted_class_idx]\n",
    "            comments_emotions[predicted_class].append(text)\n",
    "    return comments_emotions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comments_emotions = process_comments(subreddit, limit=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for emotion, comments in comments_emotions.items():\n",
    "    print(f\"Emotion: {emotion}\")\n",
    "    for comment in comments:\n",
    "        print(f\" - {comment}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
